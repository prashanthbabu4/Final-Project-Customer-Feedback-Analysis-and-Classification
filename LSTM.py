# -*- coding: utf-8 -*-
"""Untitled44.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J_MOxcbUbuLNmxHruE7fnz3M0JztDYgn
"""

import pandas as pd

df = pd.read_csv("Womens Clothing E-Commerce Reviews (2).csv")

df.isnull().sum()





# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Review Text'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Review Text'] = df['Review Text'].fillna(mode_value)



# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Title'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Title'] = df['Title'].fillna(mode_value)



# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Division Name'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Division Name'] = df['Division Name'].fillna(mode_value)


# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Department Name'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Department Name'] = df['Department Name'].fillna(mode_value)


# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Class Name'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Class Name'] = df['Class Name'].fillna(mode_value)

df.isnull().sum()

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import stopwords
import pandas as pd
import re  # Import re for regex functionality
import nltk
nltk.download('punkt_tab')

# Ensure necessary resources are downloaded
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
import nltk
nltk.download('punkt')

# Initialize the lemmatizer, stemmer, and stopwords
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Preprocessing function
def pre_procesing(text):
    if not isinstance(text, str):  # Check if the input is a string
        return ""  # Return an empty string if it's not a string (e.g., NaN or float)

    # Convert to lowercase
    text = text.lower()

    # Tokenize text
    tokens = word_tokenize(text)

    # Lemmatize and stem tokens
    tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]

    # Remove punctuation and special characters
    tokens = [word for word in tokens if word.isalnum()]

    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]

    # Join tokens back into a string
    return ' '.join(tokens)

# Assuming df is your DataFrame
# Apply the preprocessing function to the 'Review Text' column inplace
df["Review Text"] = df["Review Text"].apply(pre_procesing)

# Print first few rows of cleaned text
print(df["Review Text"].head())

def create_sentiment(rating):
    if rating <= 2:
        return 0   # Negative
    elif rating == 3:
        return 1   # Neutral
    else:
        return 2   # Positive

import pandas as pd



df["sentiment"] = df["Rating"].apply(create_sentiment)

X = df["Review Text"]
y = df["sentiment"]

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

tfidf = TfidfVectorizer(
    max_features=20000,
    ngram_range=(1, 2),
    stop_words='english'
)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf  = tfidf.transform(X_test)

tokenizer = Tokenizer(num_words=20000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_len = 200
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)

y_train_cat = to_categorical(y_train, 3)
y_test_cat = to_categorical(y_test, 3)

model = Sequential([
    Embedding(20000, 128, input_length=max_len),
    LSTM(128, return_sequences=False),
    Dropout(0.3),
    Dense(3, activation="softmax")
])

model.compile(
    loss="categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

model.summary()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical

model.fit(
    X_train_pad,
    y_train_cat,
    validation_split=0.1,
    epochs=5,
    batch_size=64
)

loss, acc = model.evaluate(X_test_pad, y_test_cat)
print("LSTM Accuracy:", acc)