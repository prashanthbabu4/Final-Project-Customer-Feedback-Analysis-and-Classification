# -*- coding: utf-8 -*-
"""Untitled45.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yg1FxUl-HZ99IqKpQaxbay6ZZSRTs4tq

#Data Set
"""

import pandas as pd

df = pd.read_csv("Womens Clothing E-Commerce Reviews (2).csv")

"""# Find Null"""

df.isnull().sum()

"""# Null Handling"""

# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Review Text'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Review Text'] = df['Review Text'].fillna(mode_value)



# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Title'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Title'] = df['Title'].fillna(mode_value)



# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Division Name'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Division Name'] = df['Division Name'].fillna(mode_value)


# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Department Name'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Department Name'] = df['Department Name'].fillna(mode_value)


# Get the most frequent value (mode) from 'Review Text'
mode_value = df['Class Name'].mode()[0]

# Fill missing values in 'Review Text' with the mode
df['Class Name'] = df['Class Name'].fillna(mode_value)

"""#  Clean and preprocess the review text


"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import stopwords
import pandas as pd
import re  # Import re for regex functionality
import nltk
nltk.download('punkt_tab')

# Ensure necessary resources are downloaded
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
import nltk
nltk.download('punkt')

# Initialize the lemmatizer, stemmer, and stopwords
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Preprocessing function
def pre_procesing(text):
    if not isinstance(text, str):  # Check if the input is a string
        return ""  # Return an empty string if it's not a string (e.g., NaN or float)

    # Convert to lowercase
    text = text.lower()

    # Tokenize text
    tokens = word_tokenize(text)

    # Lemmatize and stem tokens
    tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]

    # Remove punctuation and special characters
    tokens = [word for word in tokens if word.isalnum()]

    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]

    # Join tokens back into a string
    return ' '.join(tokens)

# Assuming df is your DataFrame
# Apply the preprocessing function to the 'Review Text' column inplace
df["Review Text"] = df["Review Text"].apply(pre_procesing)

# Print first few rows of cleaned text
print(df["Review Text"].head())





"""#Features Engneering"""

def create_sentiment(rating):
    if rating <= 2:
        return 0   # Negative
    elif rating == 3:
        return 1   # Neutral
    else:
        return 2   # Positive

import pandas as pd



df["sentiment"] = df["Rating"].apply(create_sentiment)

X = df["Review Text"]
y = df["sentiment"]

df

"""#Rating Distribution"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#See how many reviews per rating (1–5)
plt.figure(figsize=(7,5))
sns.countplot(x='Rating', data=df, palette='viridis')
plt.title("Distribution of Ratings")
plt.xlabel("Rating")
plt.ylabel("Number of Reviews")
plt.show()

"""# Recommended vs Not Recommended


"""

plt.figure(figsize=(6,4))
sns.countplot(x='Recommended IND', data=df, palette='coolwarm')
plt.title("Recommended vs Not Recommended")
plt.xlabel("Recommended (1=Yes, 0=No)")
plt.ylabel("Number of Reviews")
plt.show()

"""#Positive Feedback Count Distribution"""

plt.figure(figsize=(8,5))
sns.histplot(df['Positive Feedback Count'], bins=30, kde=True, color='green')
plt.title("Distribution of Positive Feedback Count")
plt.xlabel("Positive Feedback Count")
plt.ylabel("Frequency")
plt.show()

"""#Average Rating by Department"""

avg_rating = df.groupby('Department Name')['Rating'].mean().sort_values()

plt.figure(figsize=(10,6))
avg_rating.plot(kind='barh', color='skyblue')
plt.title("Average Rating by Department")
plt.xlabel("Average Rating")
plt.ylabel("Department")
plt.show()

"""# Word Cloud of Reviews (Visual Trend Analysis)"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

text = " ".join(review for review in df['Review Text'].dropna())

wordcloud = WordCloud(
    stopwords=STOPWORDS,
    background_color='white',
    max_words=200,
    width=800,
    height=400
).generate(text)

plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Most Frequent Words in Reviews", fontsize=18)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

"""# Data Split"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

"""#TFIDF Vectorizer"""

tfidf = TfidfVectorizer(
    max_features=20000,
    ngram_range=(1, 2),
    stop_words='english'
)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf  = tfidf.transform(X_test)

"""# LogisticRegression"""

lr_model = LogisticRegression(
    class_weight='balanced',
    max_iter=1000,
    n_jobs=-1
)

lr_model.fit(X_train_tfidf, y_train)

y_pred_lr = lr_model.predict(X_test_tfidf)

print("LOGISTIC REGRESSION RESULTS")
print("--------------------------------")
print(classification_report(y_test, y_pred_lr))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))



"""# Support Vector Classification"""

svm_model = SVC(
    kernel='linear',      # TEXT data → linear is BEST
    class_weight='balanced'
)

svm_model.fit(X_train_tfidf, y_train)

y_pred_svm = svm_model.predict(X_test_tfidf)

print("\nSVM RESULTS")
print("--------------------------------")
print(classification_report(y_test, y_pred_svm))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

"""# Ensemble Model Training & Evaluation"""

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Base models
lr_model = LogisticRegression(class_weight='balanced', max_iter=1000)
svm_model = SVC(kernel='linear', class_weight='balanced', probability=True)  # <-- probability=True

# Ensemble (soft voting)
ensemble_model = VotingClassifier(
    estimators=[
        ('lr', lr_model),
        ('svm', svm_model)
    ],
    voting='soft',
    weights=[1, 2]     # SVM slightly higher weight
)

# Train ensemble
ensemble_model.fit(X_train_tfidf, y_train)

# Predict & evaluate
y_pred = ensemble_model.predict(X_test_tfidf)

from sklearn.metrics import classification_report
print("ENSEMBLE MODEL RESULTS")
print("--------------------------------")
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
# ======================
# Imports
# ======================
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

results = pd.DataFrame({
    'Model': ['Logistic Regression', 'SVM', 'Ensemble'],
    'Accuracy': [
        accuracy_score(y_test, y_pred_lr),
        accuracy_score(y_test, y_pred_svm),
        accuracy_score(y_test, y_pred)
    ],
    'F1 Macro': [
        f1_score(y_test, y_pred_lr, average='macro'),
        f1_score(y_test, y_pred_svm, average='macro'),
        f1_score(y_test, y_pred, average='macro')
    ]
})

print("\nMODEL COMPARISON")
print("--------------------------------")
print(results)

"""# Model Comparison Table with Precision, Recall, F1"""

results = pd.DataFrame({
    'Model': ['Logistic Regression', 'SVM', 'Ensemble'],
    'Accuracy': [
        accuracy_score(y_test, y_pred_lr),
        accuracy_score(y_test, y_pred_svm),
        accuracy_score(y_test, y_pred)
    ],
    'Precision (Macro)': [
        f1_score(y_test, y_pred_lr, average='macro', labels=None, zero_division=0),
        f1_score(y_test, y_pred_svm, average='macro', labels=None, zero_division=0),
        f1_score(y_test, y_pred, average='macro', labels=None, zero_division=0)
    ],
    'Recall (Macro)': [
        f1_score(y_test, y_pred_lr, average='macro', labels=None, zero_division=0),
        f1_score(y_test, y_pred_svm, average='macro', labels=None, zero_division=0),
        f1_score(y_test, y_pred, average='macro', labels=None, zero_division=0)
    ],
    'F1 Macro': [
        f1_score(y_test, y_pred_lr, average='macro'),
        f1_score(y_test, y_pred_svm, average='macro'),
        f1_score(y_test, y_pred, average='macro')
    ]
})

print("\nMODEL COMPARISON")
print("--------------------------------")
print(results)